import pandas as pd
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_recall_curve



from autogluon.tabular import TabularPredictor

import pandas as pd
from sklearn.model_selection import train_test_split
from autogluon.tabular import TabularPredictor

if __name__ == '__main__':
    # Load your dataset
    data = pd.read_csv("data_breast.csv")
    
    # Split dataset into training and testing
    train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)
    
    # Define hyperparameters for TabPFNMix
    tabpfnmix_default = {
        "model_path_classifier": "autogluon/tabpfn-mix-1.0-classifier",
        "model_path_regressor": "autogluon/tabpfn-mix-1.0-regressor",
        "n_ensembles": 1,
        "max_epochs": 30,
    }

    hyperparameters = {
        "TABPFNMIX": [
            tabpfnmix_default,
        ],
    }

    # Define target column
    label = "5_YEAR_SURVIVAL"

    # Train the AutoGluon model
    #predictor = TabularPredictor(label=label)
    predictor = TabularPredictor(label=label, eval_metric='f1_macro').fit(
        train_data=train_data,
        presets='medium',  
        verbosity=3  
    )

    # Evaluate the model
    predictor.leaderboard(test_data, display=True)
    predictions = predictor.predict(test_data)
    true_labels = test_data['5_YEAR_SURVIVAL']  # Replace with your target column name
    report = classification_report(true_labels, predictions, target_names=["Class 0", "Class 1"])
    print(report)

    true_labels = test_data['5_YEAR_SURVIVAL']
    predicted_labels = predictor.predict(test_data)


    # Compute the confusion matrix
    cm = confusion_matrix(true_labels, predicted_labels)

    # Plot the confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Not Survived", "Survived"])
    disp.plot(cmap="Blues")
    plt.title("Confusion Matrix")
    plt.show()


    # Calculate probabilities for the positive class
    predicted_probs = predictor.predict_proba(test_data)[1]  # Probability for the positive class (survived)

    # Compute ROC curve and AUC
    fpr, tpr, _ = roc_curve(true_labels, predicted_probs)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC)')
    plt.legend(loc="lower right")
    plt.show()

    
    # Compute precision-recall values
    precision, recall, _ = precision_recall_curve(true_labels, predicted_probs)

    # Plot Precision-Recall curve
    plt.figure()
    plt.plot(recall, precision, lw=2, color='purple')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.show()

